{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'fit_prior': True, 'alpha': 0.8}, 'score': 0.96999999999999997}\n",
      "CPU times: user 20min 10s, sys: 10min 59s, total: 31min 10s\n",
      "Wall time: 33min 21s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from MongoClient import read_mongo\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "%matplotlib inline\n",
    "#%time nltk.download_shell()\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "algorithm = 'multinomialnb'\n",
    "\n",
    "def splitIntoTokens(message):\n",
    "    return TextBlob(message).words\n",
    "\n",
    "def splitIntoLemmas(message):\n",
    "    message = message.lower()\n",
    "    words = TextBlob(message).words\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    return [word.lemma for word in words]\n",
    "\n",
    "def hasUrl(message):\n",
    "    r = re.compile(r\"(http://+)|(www+)\")\n",
    "    match = r.search(message)\n",
    "    if match is None:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def splitIntoWords(text):\n",
    "    #1 remove html tags\n",
    "    # Initialize the BeautifulSoup object to strip off html tags     \n",
    "    textNoHtml = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    #2 remove numbers and punctuation\n",
    "    # Use regular expressions to do a find-and-replace\n",
    "    lettersOnly = re.sub(\"[^a-zA-Z]\",\" \",textNoHtml)\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = lettersOnly.lower().split()\n",
    "    #3 remove stop words\n",
    "    # In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(cachedStopWords)\n",
    "    woStopWords = [word for word in words if not word in stops]\n",
    "    #4 convert into base form (lemma)\n",
    "    baseForm = splitIntoLemmas(\" \" .join(woStopWords))\n",
    "    #5 Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(baseForm))\n",
    "\n",
    "def searchBestModelParameters(algorithm, trainingData):\n",
    "    if algorithm == 'multinomialnb':\n",
    "        # model the data using multinomial naive bayes\n",
    "        # define the parameter values that should be searched\n",
    "        alpha = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "        fitPrior = [True, False]\n",
    "        # specify \"parameter distributions\" rather than a \"parameter grid\"\n",
    "        paramDistribution = dict(alpha = alpha, fit_prior = fitPrior)\n",
    "        model = MultinomialNB()\n",
    "            \n",
    "    bestRun = []\n",
    "    for _ in range(1):\n",
    "        rand = RandomizedSearchCV(model, paramDistribution, cv=10, scoring = 'accuracy', n_iter = 5)\n",
    "        rand.fit(trainingData, trainingData['isSpam'])\n",
    "        # examine the best model\n",
    "        bestRun.append({'score' : round(rand.best_score_,3), 'params' : rand.best_params_})\n",
    "    print(max(bestRun, key=lambda x:x['score']))\n",
    "    return max(bestRun, key=lambda x:x['score'])\n",
    "\n",
    "def predictAndReport(algo, bestParams, train, test):\n",
    "    if algo == 'multinomialnb':\n",
    "        predictor = MultinomialNB(fit_prior = True, alpha = 0.8)\n",
    "   \n",
    "    \n",
    "    predictor.fit(train, train['isSpam'])\n",
    "    predicted = predictor.predict(test)\n",
    "    \n",
    "    dfWithClass = pd.DataFrame(predicted, columns = ['predictedClass'])\n",
    "    final = pd.concat([test, dfWithClass], axis=1)\n",
    "    #take a look at the confusion matrix\n",
    "    print(pd.crosstab(final.isSpam, final.predictedClass))\n",
    "    print(\"0s: %d, 1s: %d\" %(np.sum((final.isSpam == 0) & (final.predictedClass == 0)), np.sum((final.isSpam == 1) & (final.predictedClass == 1))))\n",
    "    print(\"Accuracy: %.3f\" %float(np.sum(final.isSpam == final.predictedClass) / float(len(test))))\n",
    "    print(\"Precision: %.3f\" %float(np.sum((final.isSpam == 1) & (final.predictedClass == 1)) / np.sum(final.isSpam == 1)))\n",
    "    \n",
    "\n",
    "#read journals\n",
    "rawJournals = read_mongo(db = 'CB', collection = 'journal', host = 'localhost')\n",
    "journals = pd.DataFrame(list(rawJournals['body']), columns = ['content'])\n",
    "journals['siteId'] = rawJournals['siteId']\n",
    "journals['text'] = rawJournals['title'].astype(str) + ' ' + journals['content']\n",
    "journals.drop(['content'], inplace = True, axis = 1)\n",
    "\n",
    "#read siteIds\n",
    "rawSite = read_mongo(db = 'CB', collection = 'site', host = 'localhost', no_id = False)\n",
    "siteIds = pd.DataFrame(list(rawSite['_id']), columns = ['siteId'])\n",
    "siteIds['isSpam'] = rawSite['isSpam']\n",
    "siteIds.isSpam.fillna(0, inplace = True)\n",
    "siteIds.rename(columns = {'isSpam':'isSiteSpam'}, inplace = True)\n",
    "\n",
    "#spam data from file\n",
    "octSiteProfileSpam = pd.read_csv(\"/Users/dmurali/Documents/spamlist_round25_from_20150809_to_20151015.csv\",\n",
    "                    usecols = ['siteId','isSpam'])\n",
    "octSiteProfileSpam.rename(columns = {'isSpam':'isOctSpam'}, inplace = True)\n",
    "\n",
    "#join the frames\n",
    "journalsFinal = journals.merge(siteIds, how='left', on = ['siteId'], sort = False).merge(octSiteProfileSpam, how='left', on = ['siteId'], sort = False)\n",
    "journalsFinal['isSpam'] = np.where(journalsFinal['isOctSpam'].isin(journalsFinal['isSiteSpam']), 1, journalsFinal['isSiteSpam'])\n",
    "journalsFinal.drop(['isOctSpam', 'isSiteSpam'], inplace = True, axis = 1)\n",
    "\n",
    "\n",
    "#clean up the text\n",
    "journalsFinal['text'].fillna(' ', inplace = True)\n",
    "journalsFinal['text'] = journalsFinal['text'].apply(splitIntoWords)\n",
    "journalsFinal['length'] = journalsFinal['text'].map(lambda text: len(text))\n",
    "\n",
    "\n",
    "#journalsFinal['hasUrl'] = journalsFinal.content.apply(has_url)\n",
    "#journalsFinal.groupby('hasUrl').describe(include=['O'])\n",
    "#journalsFinal.groupby('isSpam').describe(include=['O'])\n",
    "\n",
    "#journalsFinal.length.plot(bins=50, kind='hist')\n",
    "#journalsFinal.length.describe()\n",
    "#print(journalsFinal[journalsFinal.text == 'nan'])\n",
    "#journalsFinal.hist(column='length', by='isSpam', bins=50)\n",
    "\n",
    "#tokenize the text\n",
    "wordsVectorizer = CountVectorizer().fit(journalsFinal['text'])\n",
    "wordsVector = wordsVectorizer.transform(journalsFinal['text'])\n",
    "\n",
    "#print('sparse matrix shape:', wordsVector.shape)\n",
    "#print('number of non-zeros:', wordsVector.nnz)\n",
    "#print('sparsity: %.2f%%' % (100.0 * wordsVector.nnz / (wordsVector.shape[0] * wordsVector.shape[1])))\n",
    "#vectorAsArray = wordsVector.toarray()\n",
    "#wordSum = np.sum(vectorAsArray, axis = 0)\n",
    "#wordsByCount = zip(wordSum, wordsVectorizer.get_feature_names())\n",
    "#topByCount = sorted(wordsByCount)[:10]\n",
    "#counts = pd.DataFrame(topByCount, columns = ['Count', 'Word'])\n",
    "#print(counts)\n",
    "#sb.barplot(x = 'Word', y = 'Count', data = counts)\n",
    "inverseFreqTransformer = TfidfTransformer().fit(wordsVector)\n",
    "invFreqOfWords = inverseFreqTransformer.transform(wordsVector)\n",
    "#print(inverseFreqTransformer.idf_[wordsVectorizer.vocabulary_['get']])\n",
    "#print(inverseFreqTransformer.idf_[wordsVectorizer.vocabulary_['aaaaannnd']])\n",
    "\n",
    "weightedFreqOfWords = pd.DataFrame(invFreqOfWords.toarray())\n",
    "weightedFreqOfWords['isSpam'] = journalsFinal['isSpam']\n",
    "weightedFreqOfWords['isSpam'] = weightedFreqOfWords['isSpam'].astype(int)\n",
    "train, test, spamLabelTrain, spamLabelTest = train_test_split(weightedFreqOfWords, weightedFreqOfWords['isSpam'], test_size = 0.25)    \n",
    "#%time detector = searchBestModelParameters(algorithm, train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
